### Gradient Descent 梯度下降

这学期学了微积分，感觉对理解梯度下降是有帮助的。

之前学过，
- Loss function是一个自变量为模型函数的参数，值为误差（预估值和真值之间的误差）的函数。
- 通过找到Loss function的最小值，就可以确定模型函数的参数值。

而梯度下降就是用来找到Loss function的最小值的算法。

***

**梯度**：梯度是一个向量，代表着函数在某一点的最快变化方向的变化率，简单的理解就是在函数曲线上某个点沿着变化率最大的那个方向的斜率。
-  对于一元函数，其值就是在函数图像上某个点的一阶导数。
-  如果是多元函数，在一个点上的导数是一个向量（也就是沿着多个可变轴都会有一个斜率，沿某一个轴的斜率叫做偏导数），其中变化率最大的方向的斜率。

**梯度下降算法**就是使用当前位置的梯度来迭代计算下一个点，直到找到函数的最小值。

  <img width="201" alt="image" src="https://github.com/MaxGYX/Road2Next/assets/158791943/6e7055c5-3593-469e-8172-84a03b143186">

  -  **参数η**表示学习率，用来缩放梯度，可以控制迭代次数。
     -  学习率越小，迭代的步长就小，迭代的次数就会更多。
     -  学习率太大，可能会直接跳过最小值的点，导致无法收敛。
     -  学习率这个参数在实际使用中可以动态调整大小，来保障迭代的快一些同时不要跳过最小值。

  -  所以**进行梯度下降**的时候，需要
     -  选择迭代的起点（可以采用随机点）
     -  计算这个点的梯度，并计算下一个点的值（目标是得到最小值），直到
       - 达到设置的最大的迭代次数
       - 步长<设置的容差（即可以认为是最小值的判断标准）

  <img width="419" alt="image" src="https://github.com/MaxGYX/Road2Next/assets/158791943/54c73178-0bd8-4022-be10-1dac4ee1c08a">  

  为了避免收敛到局部最小值，可以从多个随机的开始点进行迭代。（这样就能避免最终得到的还是一个局部最小值吗？）

***

#### 一个不知道对不对的想法：
-  对于一个比较简单的函数，可以通过一阶导=0计算出所有极值的点，再通过这些点的二阶导>0（concave up，minimum）或者<0（concave down，maximum）判断是极大值还是极小值，比较所有极小值来得到最小值。
-  但是很多实际问题的函数（或者说通过多层神经网络拟合的函数）非常复杂，很难通过一阶导/二阶导公式来计算最值，所以计算机使用梯度下降的方法来迭代出最小值。（是这个原因吗？）
  
***
今天看到一个通过自然界例子理解梯度下降的形象的解释

在自然界中，**梯度下降的最好例子，就是泉水下山的过程**：
-  1.水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度的反方向为函数值下降最快的方向）；
-  2.水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）；
-  3.坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）。


