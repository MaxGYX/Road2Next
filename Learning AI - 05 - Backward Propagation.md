#### 再回顾一下神经网络训练的过程
* 先根据样本输入数据，通过随机参数计算出一个结果（前向传播），假设计算出来的结果为𝑎；
* 计算𝑎与样本标签值𝑦（即标记的真值）的差距（Loss function计算过程）；
* 更新神经元的参数（根据梯度下降计算），使用新参数再计算一次（这次计算就是有依据的向最贴近真值的方向靠近了）。
* 通过多次更新参数，最终得到让Loss最小的参数集合（即预估值和真值最贴近）；
* 所有的训练数据都执行一遍以上过程，就最终得到了使用训练数据集训练出来的参数值。

  <img width="378" alt="image" src="https://github.com/MaxGYX/Road2Next/assets/158791943/6efb4c25-8dfe-4588-bb2d-e60d1a34935a">

其中，从输入，到隐藏层神经元的计算，最终到输出，这个过程称为**前向传播“Forward Propagation”**。

而每次更新神经元参数的过程就称为**反向传播“Backward Propagation”**。之所以叫反向是因为神经网络是多层的，根据梯度下降更新参数是从最后一层先开始，一层层向前计算前面神经元的参数。全部参数计算完毕后，再正向计算使用新参数集计算的结果，进行下一次参数迭代。
  * 其中对某一个参数求偏导数使用的就是微积分的链式法则。
  * BP采用梯度下降法对参数进行更新，也就是对FP过程计算出的损失(预测值和真实值的差异)，**对所有参数求导得到梯度，然后每个参数用自身的梯度进行更新**。
